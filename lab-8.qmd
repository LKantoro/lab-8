---
title: "Lab 8"
subtitle: "Cheese Gromit!"
editor: source
---

> **Goal:** Scrape information from <https://www.cheese.com> to obtain a dataset
> of characteristics about different cheeses, and gain deeper insight into your
> coding process. ðŸª¤

**Part 1:** Locate and examine the `robots.txt` file for this website. Summarize
what you learn from it.


- The only lines on 'https://www.cheese.com/robots.txt' is User-agent: * and Sitemap: https://www.cheese.com/sitemap.xml .

- 'User-agent: *' informs us that anyone is allowed to scrape.

- The lack of Crawl-delay, Visit-time and Request-rate means that we can scrape as often as we want, at any time of day, from as many different users as we want.

- The lack of a 'Disallow" section means that there are no scraping restrictions on specific areas of the cheese website.


**Part 2:** Learn about the `html_attr()` function from `rvest`. Describe how
this function works with a small example.


```{r}
library(rvest)
```

- The html_attr() function obtains the value of an attribute. It is frequently used after html_elements on a website. 

- Below, html_elements("a") extracts all elements inside the <a> attribute. html_attr("href") extracts the value of href inside the <a> element. In this case the href is the link to the wikipedia article on cats.

- Example below can be found [here](https://cran.r-project.org/web/packages/rvest/vignettes/rvest.html).


```{r}
html <- minimal_html("
  <p><a href='https://en.wikipedia.org/wiki/Cat'>cats</a></p>
  <img src='https://cataas.com/cat' width='100' height='200'>
")

html %>% 
  html_elements("a") %>% 
  html_attr("href")
```


**Part 3:** (Do this alongside Part 4 below.) I 
used [ChatGPT](https://chat.openai.com/chat) to start the process of scraping
cheese information with the following prompt:

> Write R code using the rvest package that allows me to scrape cheese
> information from cheese.com.

Fully document your process of checking this code. Record any observations you
make about where ChatGPT is useful / not useful.

```{r}
#| eval: false
#| label: small-example-of-getting-cheese-info_unedited

# Load required libraries
library(rvest)
library(dplyr)

# Define the URL
url <- "https://www.cheese.com/alphabetical"

# Read the HTML content from the webpage
webpage <- read_html(url)

# Extract the cheese names and URLs
cheese_data <- webpage %>%
  html_nodes(".cheese-item") %>%
  html_nodes("a") %>%
  html_attr("href") %>%
  paste0("https://cheese.com", .)

cheese_names <- webpage %>%
  html_nodes(".cheese-item h3") %>%
  html_text()

# Create a data frame to store the results
cheese_df <- data.frame(Name = cheese_names,
                        URL = cheese_data,
                        stringsAsFactors = FALSE)

# Print the data frame
print(cheese_df)
```

- The url found the list of cheeses on the website and read it in using read_html, which is a good start.

- Everything in the code chunk below works well.

```{r}
#| eval: false


# Load required libraries
library(rvest)
library(dplyr)

# Define the URL
url <- "https://www.cheese.com/alphabetical"

# Read the HTML content from the webpage
webpage <- read_html(url)
```

- ChatGPT's errors begin in the code chunk below

- cheese_data is attempting to check all cheese items and extract their urls. However, the html_nodes(".cheese-item") is not correctly finding all cheeses, which makes the rest of the function produce only "https://cheese.com" without specific cheeses appended at the end of the url.

- Running ' webpage %>% html_nodes(".cheese-item")' produces a warning saying it could not find the child attributes.

- cheese_names is supposed to be the extracting names of the cheese from each url. Since cheese_data is not a list of cheese url's like it is supposed to be, cheese_names does not contain a list of cheese names.

- The cheese_df object fails because cheese_data contiains one url, and cheese_names is empty since it didn't find any cheeses.

```{r}
# Extract the cheese names and URLs
cheese_data <- webpage %>%
  html_nodes(".cheese-item") %>%
  html_nodes("a") %>%
  html_attr("href") %>%
  paste0("https://cheese.com", .)

cheese_names <- webpage %>%
  html_nodes(".cheese-item h3") %>%
  html_text()

# Create a data frame to store the results
cheese_df <- data.frame(Name = cheese_names,
                        URL = cheese_data,
                        stringsAsFactors = FALSE)

# Print the data frame
print(cheese_df)
```






**Part 4:** Obtain the following information for **all** cheeses in the
database:

-   cheese name
-   URL for the cheese's webpage (e.g., <https://www.cheese.com/gouda/>)
-   whether or not the cheese has a picture (e.g., 
[gouda](https://www.cheese.com/gouda/) has a picture, but 
[bianco](https://www.cheese.com/bianco/) does not).

To be kind to the website owners, please add a 1 second pause between page
queries. (Note that you can view 100 cheeses at a time.)

**Part 5:** When you go to a particular cheese's page (like 
[gouda](https://www.cheese.com/gouda/)), you'll see more detailed information
about the cheese. For [**just 10**]{.underline} of the cheeses in the database,
obtain the following detailed information:

-   milk information
-   country of origin
-   family
-   type
-   flavour

(Just 10 to avoid overtaxing the website! Continue adding a 1 second pause
between page queries.)

**Part 6:** Evaluate the code that you wrote in terms of **efficiency**. To
what extent do your function(s) adhere to the **principles for writing good functions**?
To what extent are your **functions efficient**? To what extent is your 
**iteration of these functions efficient**? 